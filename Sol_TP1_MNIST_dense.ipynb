{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning avec les modules Python tensorflow2/keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement / exploitation d'un réseau de neurones dense pour la reconnaissance de chiffres manuscrits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La banque d'images MNIST\n",
    "\n",
    "Dans ce TP nous utilisons les images labelisées de la banque MNIST disponible sur Internet (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "La banque MNIST contient 70000 images en ton de gris de 28 $\\times$ 28 pixels, correspondant à des matrice 28 $\\times$ 28 de nombres `uint8` (entiers positifs dans l'intervalle [0 ; 255]).<br>\n",
    "Les 70000 images du MNIST sont regoupées en un jeu de **60000 images d'apprentissage** et un jeu de **10000 images de test**.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "L'évaluation des performances d'un réseau entraîné doit se faire sur un jeu de données différent du jeu d'entraînement : c'est pour cela que la banque MNIST propose 10000 images de test différentes des 60000 images d'entraînement.\n",
    "</div>\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "<img src=\"img/MNIST_digits_sample.png\" width=\"500\"><br>\n",
    "[crédit image : JLC]\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/ Quelques rappels...\n",
    "\n",
    "## Le neurone artificiel\n",
    "C'est une unité de traitement informatique qui reçoit des entrées et calcule la valeur de sa **fonction d'activation** en un point défini par la **combinaison linéaire de ses entrées** $\\sum_i \\omega_i x_i - b$ :\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"./img/nn_1.png\" width=\"600\" ><br>\n",
    "    [crédit image : JLC]\n",
    "</p>\n",
    "\n",
    "L'entrée *Bias* reçoit le stimuli `-1` affecté du poids $b$ : il permet de décaler le point où la fonction d'activation est calculée.\n",
    "\n",
    "## Fonction d'activation\n",
    "\n",
    "Principaux rôles :\n",
    "\n",
    "- introduire dans le neurone un comportement **non linéaire** (comme des mécanismes de seuil, de saturation...)\n",
    "- fixer la plage de sortie de la valeur calculée par le neurone, par exemple dans l'intervalle $[-1 ; 1]$, $[0 ; 1]$ ou encore $[0 ; +\\infty[$\n",
    "\n",
    "Exemples de fonctions d'activations couramment utilisées :<br />\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"./img/activationFunctions.png\" width=\"950\"><br>\n",
    "    [crédit image : JLC]\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B/ Structure du réseau dense à construire \n",
    "Dans ce premier TP,  nous allons construire un **réseau dense**, avec :\n",
    "- une **couche d'entrée** de 784 valeurs comprises entre 0 et 1 (les pixels des image MNIST 28 $\\times$ 28 mis sous forme d'un vecteur de 784 nombres `float`),\n",
    "- une **couche cachée** de 784 neurones utilisant la fonction d'activation `relu`,\n",
    "- une **couche de sortie** à 10 neurones, pour la classification des images en 10 classes associées aux chiffres {0,1,2...9}, utilisant la fonction d'activation `softmax` adaptée aux problèmes de classification.\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/archiReseau.png\" alt=\"archiReseau.png\" style=\"width: 900px;\"><br> \n",
    "    [crédit image : JLC]\n",
    "</p>\n",
    "\n",
    "Remarques : \n",
    "- Chaque neurone de la première couche cachée reçoit 785 entrées : les 784 valeurs $x_i$ des pixels de l'image plus le biais (l'entrée '-1'). \n",
    "- $\\leadsto$ Il y a donc 785 inconnues pour chaque neurone : les 784 poids $w_i$ affectés à chaque entrée $x_i$, plus le  poids $b$ affecté au biais.\n",
    "- $\\leadsto$ on compte donc 785 $\\times$ 784 inconnues pour la couche cachée et 785 $\\times$ 10 inconnues pour la couche de sortie : soit un total de 623290 inconnues dont la valeur doit être optimisée par l'algorithme d'apprentissage du réseau.\n",
    "\n",
    "### Fonction d'activation softmax\n",
    "\n",
    "La fonction `softmax` calcule pour chaque neurone de sortie $k$ la valeur $\\displaystyle{Y_k = \\frac{e^{y_k}}{\\sum_i{e^{y_i}}}}$, où $y_k$ désigne la combinaison linéaire $\\sum_i \\omega_i x_i - b$ calculée par le neurone $k$.\n",
    "\n",
    "\n",
    "`softmax` associe ainsi à chacune des sorties $y_k$ une valeur $Y_k \\in [0, 1]$ qui peut être interprétée comme la probablité de la sortie $k$ : on obtient une valeur proche de 1 pour le neurone fournissant la valeur $y_i$ la plus grande, et quasiment 0 pour tous les autres.\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/softmax.png\" width=\"400\"><br>\n",
    "    [crédit image : JLC]\n",
    "</p>\n",
    "\n",
    "\n",
    "### Catégorisation des labels (*one-hot coding*)\n",
    "\n",
    "Les labels des images sont des entiers entre 0 et 9 désignant les chiffres de '0' à '9' alors que la sortie du réseau est un ensemble de 10 valeurs `float` comprises entre 0 et 1 (un vecteur de 10 `float`).<br>\n",
    "Pour faciliter la comparaison entre le label associé à une image et la sortie du réseau calculée pour cette image, on utilise la catégorisation *one-hot coding* qui associe à un scalaire un vecteur dont les éléments sont tous nuls sauf un.\n",
    "Dans le cas des 10 classes correspondant aux labels '0' à '9', la représentation _one-hot coded_ donne :\n",
    "\n",
    "| chiffre|   | $Y'_i$ : vecteur _one-hot_     |\n",
    "|:-------|---|----------------------:|\n",
    "| 0      |   | [1 0 0 0 0 0 0 0 0 0] |\n",
    "| 1      |   | [0 1 0 0 0 0 0 0 0 0] |\n",
    "| 2      |   | [0 0 1 0 0 0 0 0 0 0] |\n",
    "| 3      |   | [0 0 0 1 0 0 0 0 0 0] |\n",
    "| 4      |   | [0 0 0 0 1 0 0 0 0 0] |\n",
    "| 5      |   | [0 0 0 0 0 1 0 0 0 0] |\n",
    "| 6      |   | [0 0 0 0 0 0 1 0 0 0] |\n",
    "| 7      |   | [0 0 0 0 0 0 0 1 0 0] |\n",
    "| 8      |   | [0 0 0 0 0 0 0 0 1 0] |\n",
    "| 9      |   | [0 0 0 0 0 0 0 0 0 1] |\n",
    "\n",
    "### Calcul de l'erreur d'inférence du réseau\n",
    "\n",
    "Pendant la phase d'entraînement du réseau, chaque image en entrée du réseau donne un vecteur de probabilités en sortie (inférence calculée par le réseau) qui peut être comparé à la représentation *hot-one* du label associé à l'image d'entrée.\n",
    "\n",
    "On peut utiliser l'erreur *cross entropy*  ($-\\sum_i{Y'_i.\\log{Y_i}}$) qui mesure l'écart entre la représentation *one-hot* du label et la réponse du réseau :\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/CrossEntropy.png\" width=600><br>\n",
    "    [crédit image : vidéo \"Deep Learning TensorFlow\" de Martin Gorner]\n",
    "</p>\n",
    "\n",
    "\n",
    "### Backpropagation error\n",
    "\n",
    "L'algorithme de **rétropropagation de l'erreur** permet de modifier les poids des couches du réseau pour minimiser l'erreur entre _valeur attendue_ et _valeur prédite_ par le réseau. Au fur et à mesure des apprentissages successifs, les poids du réseau convergent vers un état qui minimize l'erreur d'inférence du réseau et qui constitue l'état du réseau entraîné.\n",
    "\n",
    "### Optimiseur\n",
    "\n",
    "La recherche du minimum de la fonction de coût est confié à un **optimiseur** :  un des algorithmes les plus simples est la __descente de gradient__ (GD), qui consiste à se déplacer dans le sens de la pente la plus forte à chaque itération :\n",
    "\n",
    "* Sous quelle hypothèse cet algorithme permet-il de trouver le minimum global de la fonction de coût selon vous ?\n",
    "\n",
    "* Pensez vous que cette hypothèse soit vérifiée pour les réseaux de neurones ?\n",
    "\n",
    "* Que se passe-t-il si cette hypothèse n'est pas vérifiée ?\n",
    "\n",
    "__Adam__ est un optimiseur plus complexe que __GD__.  Sur l'animation ci-dessou on voit plusieurs optimiseurs se déplacer sur une fonction de coût ne comportant que 2 paramètres (au lieu de plusieurs milliers...) à la recherche d'un minimum. \n",
    "\n",
    "Concentrez-vous sur Adam et GD : quelle semble être la caractéristique de Adam comparée a GD ?\n",
    "\n",
    "_(Numbers in figure legend indicate learning rate, specific to each Optimizer.)_\n",
    "![cette image](https://github.com/Jaewan-Yun/optimizer-visualization/raw/master/figures/movie11.gif)\n",
    "\n",
    "_[source : github.com/Jaewan-Yun/optimizer-visualization/raw/master/figures/movie11.gif]_\n",
    "\n",
    "\n",
    "Une autre caracteristique de l'algorithme GD, est que le pas effectué à chaque itération est fixe. \n",
    "L'image suivante montre Adam et GD dans un cas ou la pente devient trés forte :\n",
    "\n",
    "* GD arrive-t-il à converger ? Comprenez vous pourquoi ?\n",
    "\n",
    "* Adam ne semble pas soumis au même problême que GD ? Quelle autre caractéristique de Adam cela montre t-il ?\n",
    "\n",
    "![cette image](https://github.com/Jaewan-Yun/optimizer-visualization/raw/master/figures/movie9.gif)\n",
    "_(source : github.com/Jaewan-Yun/optimizer-visualization/raw/master/figures/movie9.gif)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C/ Travail à faire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnement Python de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:normal\"> \n",
    "L'état de l'art actuel des projets de machine learning sous Python préconise l'utilisation d'un <span style=\"font-weight:bold;\">environnement virtuel Python</span> qui permet de maîtriser pour chaque projet les versions de l'interpréteur et des modules Python \"sensibles\" (comme tensorflow par exemple).\n",
    "\n",
    "Dans le cas d'un démarrage de l'ordinateur avec une clef USB Ubuntu, on peut considérer que la clef fournit un environnement Python dédié, à condition de ne pas faire de mises à jour des paquets Python avec <span style=\"font-style:italic\">pip install...</span>\n",
    "    \n",
    "Dans le cas contraire, la <A href=\"https://learn.ros4.pro/fr/faq/Python/\">FAQ Python</A> explique comment créer et utiliser un EVP basé sur **miniconda3** pour utiliser numpy et tensorflow2 avec la bibliothèque optimisée <A href=\"https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html\">MKL</A>.\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation tensorflow/keras\n",
    "\n",
    "Le module **keras** qui permet une manipulation de haut niveau des objets **tensorflow** est intégré dans tensorflow2. <br>\n",
    "La documentation du module **tf.keras** à consulter pour ce TP est ici : https://www.tensorflow.org/api_docs/python/tf/keras. \n",
    "\n",
    "Versions des modules Python validées pour ce TP sous Ubuntu 20 / Python3.8.5 :\n",
    "- tensorflow 2.4.0 incluant tensorflow.keras 2.4.0\n",
    "- OpenCV >= 4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys, cv2\n",
    "print(f\"Python    : {sys.version.split()[0]}\")\n",
    "print(f\"tensorflow: {tf.__version__} incluant keras {keras.__version__}\")\n",
    "print(f\"OpenCV    : {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incrustation des tracés matplotlib dans le notebook et import de modules utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproductibilité des générateurs pseudo-aléatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les poids d'un réseau de neurones sont initialisés par tirage aléatoire ; lorsqu'on invoque plusieurs fois de suite des générateurs pseudo-aléatoires on obtient une séquence aléatoire différente à chaque invocation. Par exemple avec `numpy.random.rand` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.random.rand(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.rand(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce comportement peut être gênant pour étudier les performances d'un réseau entraîné en fonction des paramètres de construction ou d'entraînement : à chaque fois qu'on exécute la cellule ou le notebook avec de nouvelles valeurs des paramètres, on risque d'obtenir un état initial du réseau différent, conduisant à un état du réseau entraîné différent... ce qui peut gêner les comparaisons.\n",
    "\n",
    "La technique pour éviter ce phénomène consiste à fixer la **graine** (*seed*) des générateurs aléatoires de façon à produire des **séquences aléatoires reproductibles**.<br>\n",
    "\n",
    "Les cellules suivantes illustrent le mécanisme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer ce mécanisme aux calculs faits avec tensorflow, on pourra utiliser la fonction `set_seed` (cf page [tf.random.set_seed](https://www.tensorflow.org/api_docs/python/tf/random/set_seed)) :\n",
    "**`tf.random.set_seed(SEED)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Récupération des images MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulter la documentation de la fonction `load_data` sur la page [tf.keras.datasets.mnist.load_data](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data) puis compléter la cellule ci-dessous pour charger les données du MNIST en nommant les données renvoyées :<br>\n",
    "- `im_train`, `lab_train` pour les images et les labels d'entraînement,\n",
    "- `im_test`, `lab_test` pour les images et les labels de test.\n",
    "\n",
    "(En cas de message d'erreur de type _\"SSL error....\"_ pour téléchager les données du MNIST, voir [Python SSL Certification Problems in Tensorflow](https://stackoverflow.com/questions/46858630/python-ssl-certification-problems-in-tensorflow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(im_train, lab_train), (im_test, lab_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule ci-dessous affiche les attributs `shape` et `dtype` des tableaux numpy obtenus : les valeurs sont-elles cohérentes ? pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"im_train -> shape:\", im_train.shape, \", dtype:\", im_train.dtype,)\n",
    "print(\"im_test  -> shape:\", im_test.shape,  \", dtype:\", im_test.dtype,)\n",
    "print(\"lab_train-> shape:\", lab_train.shape,  \", dtype:\", lab_train.dtype)\n",
    "print(\"lab_test -> shape:\", lab_test.shape,  \", dtype:\", lab_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des images et des étiquettes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la fonction `imshow` du module `matplotlib.pyplot` faire afficher en tons de gris la 601-ème image du tableau `im_train`.<br>\n",
    "Indications :\n",
    "- utiliser `plt.figure(figsize=(2,2))` pour fixer la taille de l'image\n",
    "- utiliser l'option `cmap='gray'` de `imshow` pour l'affichage en ton de gris\n",
    "- enlever les graduations en X et Y (par exemple avec `plt.xticks([]); plt.yticks([])`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(im_train[600], cmap='gray')\n",
    "plt.xticks([]); plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que le label MNIST associé à `im_train[600]` correspond bien à ce qu'on voir sur l'image...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_train[600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `plot_images` définie ci-dessous prends les arguments `(images, r, L, C)` : elle affiche les images du tableau `images` (tableau de matrices) dans une grille d'images de `L` lignes et `C` colonnes  en commençant avec l'image de rang `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, r,L,C):\n",
    "    plt.figure(figsize=(C,L))\n",
    "    for i in range(L*C):\n",
    "        plt.subplot(L, C, i+1)\n",
    "        plt.imshow(images[r+i], cmap='gray')\n",
    "        plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire afficher les images d'entraînement à partir de la 601 ème dans une grille de 4 x 6 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(im_train, 600, 4, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la cellule ci-dessous, on trie les images et on affiche des lignes de '0', des lignes de '1'... en noir sur fond blanc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = []\n",
    "for i in range(10):\n",
    "    indexes = np.where(lab_train == i) # indices des labels égaux à i\n",
    "    i_data = im_train[indexes][:16]    # tableau des images correspondates\n",
    "    data.append(i_data)\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "L, C = len(data), len(data[0])\n",
    "for row, digits in enumerate(data):\n",
    "    for col, digit in enumerate(digits):\n",
    "        plt.subplot(L, C , row*C + col + 1)\n",
    "        plt.imshow(255-digit, cmap='gray')\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        plt.box(False)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitement des données MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour adapter les données à la couche d'entrée du réseau de neurones, on transforme les matrice d'entiers `uint8` représentant les images 28x28 pixels en vecteurs **normalisés** $(V_i)_{i=0..783}$ de 784 valeurs réelles $V_i$  avec $ 0 \\leqslant V_i \\leqslant 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation des matrices d'entrée en vecteurs normalisés :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour éviter de \"claquer en dur\" le nombre d'images d'entraînement et de test ainsi que la dimension des images, on récupère ces paramètres :\n",
    "- avec l'attribut `shape` des tableaux `im_train` et `im_test`\n",
    "- avec l'attribut `size` de la première image d'entraînement par exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_im_train = im_train.shape[0]    # nombre d'images d'entraînement\n",
    "nb_im_test  = im_test.shape[0]     # nombre d'images de test \n",
    "nb_pixel    = im_train[0].size     # nombre d'éléments (des pixels) de la première image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{nb_im_train} images d'entraînement et {nb_im_test} images de test\")\n",
    "print(f\"{nb_pixel} pixels dans chaque image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir maintenant les tableaux `x_train` et `x_test` contenant les matrices des tableaux `im_train` et `im_test` *mises à plat* sous forme de vecteurs normalisés (valeurs comprises entre 0 et 1).<br>\n",
    "*indications* : utiliser la méthode `reshape` des tableaux ndarray de numpy et les paramètres `nb_im_train`, `nb_im_test` et `nb_pixel` calculés précédemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = im_train.reshape(nb_im_train, nb_pixel)/im_train.max()\n",
    "x_test  = im_test.reshape(nb_im_test, nb_pixel)/im_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier les dimensions des tableaux `x_train` et `x_test` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que les valeurs min et max des tableaux `x_train` et `x_test` sont bien celles attendues :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.min(), x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.min(), x_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Codage *one-hot*  des labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulter la documentation de la fonction `to_categorical` sur la page [tf.keras.utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)  et en déduire comment définir les tableaux `y_train` et `y_test` contenant la version encodée *hot-one* des tableaux `lab_train` et `lab_test` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 'one-hot' encoding' des labels :\n",
    "y_train = to_categorical(lab_train)\n",
    "y_test  = to_categorical(lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier visuellement les 10 premières valeurs des tableaux `lab_train` et `y_train` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Construction du réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant construire un réseau de neurones **séquentiel** en **5 lignes** Python à l'aide du module **keras**.\n",
    "\n",
    "Après lecture de la page [guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model), construire le réseau  de façon incrémentale (méthode `add`) dans la cellule ci-dessous en suivant la démarche proposée :\n",
    "- 1/ Créer l'objet `model` instance de la classe `Sequential` (cf [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)).\n",
    "- 2/ Ajouter la couche d'entrée `Input(shape=<nombre de neurones>)` (cf [tf.keras.layers.InputLayer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer)) pour spécifier la forme des données d'entrée. Utiliser le paramètre `nb_pixels`...<br>\n",
    "- 3/ Ajouter les autres couches `Dense` (cf [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) :\n",
    "    - couche 1 : `Dense(<nombre de neurones>, activation='relu')` (cf [tf.keras.activation.relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu))\n",
    "    - couche 2 : `Dense(<nombre de neurones>, activation='softmax')` (cf [tf.keras.activation.softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax)).<br>\n",
    "On pourra utiliser les paramètres `nb_pixels` et `nb_classes` pour indiquer le nombre de neurones sans les 'claquer en dur'...    \n",
    "- 4/ Une fois construit, le réseau doit être compilé (au sens de tensorflow) avec la méthode `compile` et les arguments :\n",
    "    - `loss='categorical_crossentropy'` : choix de la fonction d'erreur (cf [tf.keras.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy))\n",
    "    - `optimizer='adam'` : choix de l'optimiseur Adam (cf page [tf.keras.optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam))\n",
    "    - `metrics=['accuracy']` pour obtenir les données permettant de tracer les courbes de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "nb_classe = 10\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# les 5 lignes pour construire le réseau de neurones:\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(nb_pixel,), name='Input'))\n",
    "model.add(Dense(nb_pixel, activation='relu', name='C1'))\n",
    "model.add(Dense(nb_classe, activation='softmax', name='C2'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la méthode `summary` de l'objet `model`, faire afficher la description du modèle et vérivhier les dimensions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pourquoi y-a-t-il des `None` dans la colonne \"Output Shape\"  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrouver le nombre total de paramètres avec une formule simple ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "785*784+785*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `plot_model` permet de dessiner la structure du réseau (voir la page [tf.keras.utils.plot_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model)).<br>\n",
    "Faire tracer la structure du modèle en ajoutant l'option `show_shapes=True` à l'appel de `plot_model` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remarque* : en utilisant l'argument nommé `name` dans la fonction `Dense`, on peut donner des noms personnalisés aux couches, qui apparaîtront dans les sorties de `summary` et de `plot_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde de l'état initial du  réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut sauvegarder l'état initial des poids du réseau non-entraîné (valeurs aléatoires) avec la méthode `save_weights` de la classe `Sequential`. <br>\n",
    "Ce sera utile plus loin pour remettre le réseau à son état initial avant de relancer d'autres entraînements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# vérifier que le dossier 'weights' existe et sinon le créer:\n",
    "if not os.path.isdir(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# sauvegarde des poinds du réseau initial:\n",
    "key = 'model_initial'\n",
    "model.save_weights('weights/'+key)\n",
    "\n",
    "# afficher les fichiers créés:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : la méthode `save_weights` utilise la partie `key` du chemin passé en argument pour préfixer les fichiers créés.<br>\n",
    "Lors de la lecture ultérieure des poids du réseau avec la méthode `load_weights` de la classe `Sequential`, il suffira de donner la même information pour retrouver les bons fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Entraînement du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulter au besoin la documentation de la méthode `fit` sur la page [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "Compléter la cellule ci-dessous pour entraîner le réseau avec la méthode `fit` de l'objet `model` en utilisant les arguments :\n",
    "- `x_train` : les 60000 images mises à plat et normalisées\n",
    "- `y_train` : les 60000 labels encodés *one-hot*.\n",
    "- `epochs=15` : faire 15 fois l'entraînement complet.\n",
    "- `batch_size=128` : découper le jeu des données d'entrée (les 60000 images) en \"lots\" (*batch*) de taille `batch_size` (ici en lots de 128 images).<br>\n",
    "La mise à jour des poids du réseau est faite au bout de `batch_size` échantillons d'entrée.<br>\n",
    "La valeur de `batch_size` (par défaut : 32) est un paramètre qui influe sur la qualité de l'apprentissage mais aussi sur son empreinte mémoire : on pourra plus loin essayer différentes (64, 128, 256 ...) et observer comment évolue la qualité de l'entraînement).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# au cas on on exécute plusieurs fois cette cellule, on peut ré-initialiser \n",
    "# le réseau à son état initial si on veut comparer les entraînements...\n",
    "key = 'model_initial'\n",
    "model.load_weights('weights/'+key)  \n",
    "\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 epochs=15, \n",
    "                 batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet `hist` retourné par la méthode `fit` possède un attribut `history` de type `dict` dont les clefs `'loss'` et `'accuracy'` sont associées aux informations correspondantes obtenues après chaque _epoch_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracé des courbes `loss` et `accuracy` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `plot_loss_accuracy` du module `utils.tools` (présent dans le répertoire du notebook) permet de tracer les courbes \"fonction d'erreur\" et \"précision\" sur la base des données stockées dans `hist`.<br> Importer et utiliser la fonction`plot_loss_accuracy` pour faire tracer ces courbes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### À propos de la reproductibilité de l'entraînement..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la reproductibilité parfaite d'un entraînement de réseau de neurones est parfois difficile à atteindre avec tensorflow....\n",
    "\n",
    "Observe les résultats des cellules suivantes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans ce premier essai, le réseau est initialisé une fois, puis à chaque tour de boucle il est entraîné sur la base de son état au tour de boule précédent... `loss` et `accuracy` évoluent à chaque tour de boucle.<br>\n",
    "$\\leadsto$ c'est comme si on avait fait un `model.fit(x_train, y_train, epochs=10, batch_size=64)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recharger l'état initial du réseau:\n",
    "key = 'model_initial'\n",
    "model.load_weights('weights/'+key) \n",
    "\n",
    "for _ in range(10):\n",
    "    hist = model.fit(x_train, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, on réinitialise bien l'état initial du réseau **avant chaque lancement de `fit`** : c'est mieux, mais ce n'est pas encore parfait..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # recharger l'état initial du réseau:\n",
    "    key = 'model_initial'\n",
    "    model.load_weights('weights/'+key) \n",
    "\n",
    "    hist = model.fit(x_train, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et dans cet essai, on ré-initialise la graine tensorflow avec `tf.random.set_seed(SEED)` avant chaque invocation de `fit` : ce n'est toujours pas exactement les mêmes valeurs `loss` et `accuracy` que l'on observe, mais on en est très proche..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # recharger l'état initial du réseau:\n",
    "    key = 'model_initial'\n",
    "    model.load_weights('weights/'+key) \n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    hist = model.fit(x_train, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Entraînement du réseau avec test à chaque *epoch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir un indicateur réaliste de la qualité du réseau entraîné on peut tester à chaque `epoch` la précison des inférences du réseau entraîné en utilisant les données de test : il suffit de passer l'agument `validation_data` à la méthode `fit`, en lui affectant le tuple des données de test `(x_test, y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recharger l'état initial du réseau:\n",
    "key = 'model_initial'\n",
    "model.load_weights('weights/'+key) \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test), \n",
    "                 epochs=15, \n",
    "                 batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a cette fois dans `hist.history` non seulement les clefs `loss` et `accuracy` mais aussi `val_loss` et `val_accuracy` calculées avec les données de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire afficher ces courbes avec la fonction `plot_loss_accuracy` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que la précision calculée avec les données de test tend vers une limite voisine de 98%. <br>\n",
    "On pourrait penser qu'augmenter la valeur de `epochs` permet d'améliorer la précision du réseau... mais on court le risque de sur-entraînner le réseau (*over-fit*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrêter automatiquement l'entraînement avant *over-fit*\n",
    "\n",
    "Keras propose des outils pour arrêter automatiquement l'apprentissage en surveillant par exemple la croissance de la précision d'une `epoch` à l'autre (cf le callback  [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)).\n",
    "\n",
    "\n",
    "On peut ainsi définir une liste de fonctions *callback* que l'on peut passer en argument à la fonction `fit` avec l'agument nommé  `callbacks` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_accuracy',  # la grandeur à surveiller\n",
    "                  patience=2,              # accepter que 'val_accuracy' diminue 2 fois en tout\n",
    "                  mode='max',              # arrêter si le paramètre décroît \n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# recharger l'état initial du réseau:\n",
    "key = 'model_initial'\n",
    "model.load_weights('weights/'+key) \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15, \n",
    "                 batch_size=128, \n",
    "                 callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu de surveiller la croissance du paramètre `val_accuracy`, on peut surveiller la décroissance de `val_loss` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_loss',  # la grandeur à surveiller\n",
    "                  patience=2,          # accepter que 'val_accuracy' diminue 2 fois en tout\n",
    "                  mode='min',          # arrêter si le paramètre croît \n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# recharger l'état initial du réseau:\n",
    "key = 'model_initial'\n",
    "model.load_weights('weights/'+key) \n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15, \n",
    "                 batch_size=128, \n",
    "                 callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder les poids du  réseau entraîné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour sauvegarder les **poids** d'un réseau entraïné dans un fichier, on utilise la méthode `save_weights` de la classe `Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# vérifier que le dossier 'weights' existe et sinon le créer:\n",
    "if not os.path.exists(\"weights\"): os.mkdir(\"weights\")\n",
    "\n",
    "# sauvegarde des poids du réseau entrainé:\n",
    "key = 'model_trained'\n",
    "model.save_weights('weights/'+key)\n",
    "\n",
    "# afficher les fichiers créés:\n",
    "files=[os.path.join(\"weights\",f) for f in os.listdir(\"weights\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder la structure du réseau et ses poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `save` de la classe `Sequential` permet d'enregistrer dans des fichiers **toute la structure et les poids** du réseau entraïné.<br />\n",
    "$\\leadsto$ Ceci permet de recréer plus tard *from scratch* le réseau entrainé pour passer en phase exploitation du réseau par exemple, en utilisant la fonction`tf.keras.models.load_model` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# vérifier que le dossier 'weights' existe et sinon le créer:\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# sauvegarder structure réseau + poids :\n",
    "key = 'model'\n",
    "model.save('models/'+key) \n",
    "\n",
    "# afficher les fichiers créés:\n",
    "files=[os.path.join(\"models\",f) for f in os.listdir(\"models\") if f.startswith(key)]\n",
    "for f in files: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6/ Exploitation du réseau avec le jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `predict` de l'objet `model` permet de calculer les inférences du réseau pour une ou plusieurs entrées (voir la méthode `predict`dans la page [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)).\n",
    "\n",
    "La cellule ci-dessous montre la mise en oeuvre de la méthode `predict`, et comment exploiter la représentation  *one-hot* renvoyée par `fit` en utilisant la méthode `argmax` des tableaux de numpy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100   # numéro image de test\n",
    "rep = model.predict(x_test[i:i+1]) # Attention: x doit être un tableau de matrices...\n",
    "                                   # => x[i] ne convient pas !\n",
    "\n",
    "print(f\"sortie one-hot du réseau pour l'image de rang {i} :\\n{rep[0]}\")\n",
    "\n",
    "# limiter l'affichage des composantes des tableaux numpy à 1 chiffre :    \n",
    "with np.printoptions(formatter={'float':'{:.1f}'.format}):    \n",
    "    print(f\"\\nsortie one-hot réseau arrondie à 1 chiffre : {rep[0]}\")\n",
    "    \n",
    "print(f\"rep[0].argmax() donne : {rep[0].argmax()}\")\n",
    "\n",
    "print(f\"\\nLa bonne réponse est {lab_test[i]} soit en 'one-hot' : {y_test[i]}\")\n",
    "\n",
    "plot_images(im_test,i,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilité de la méthode `ndarray.argmax` de numpy pour décoder le tableau de vecteurs *one-hot* renvoyé par la méthode `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand on calcule une inférence du réseau `model` avec les données de test par exemple, on obtient un résultat qui est un tableau de vecteurs codés *one-hot*, comme le détaille la cellue suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)\n",
    "\n",
    "print(\"forme du tableau 'results':\", results.shape)\n",
    "print(\"allure des vecteurs du tableau 'result', par exemple :\")\n",
    "with np.printoptions(formatter={'float':'{:.4f}'.format}): \n",
    "    print(\"results[0]  :\", results[0])\n",
    "    print(\"results[-1] :\", results[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En écrivant `results.argmax(axe=-1)`, on obtient le tableau des `argmax` de chaque vecteur -> c'est directement le tableau des chiffres reconnus par le réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiffres_reconnus = results.argmax(axis=-1)\n",
    "\n",
    "print(\"chiffres_reconnus -> shape:\", chiffres_reconnus.shape, \", dtype:\", chiffres_reconnus.dtype)\n",
    "print(f\"contenu de chiffres_reconnus : {chiffres_reconnus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrouver le taux de réussite du réseau entrainé en utilisant les inférences du réseau pour les entrées `x_test` et les labels `lab_test` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)\n",
    "chiffres_reconnus = results.argmax(axis=-1)\n",
    "\n",
    "success = 0\n",
    "for chiffre, label in zip(chiffres_reconnus, lab_test):\n",
    "    success += (chiffre == label)\n",
    "print(f\"taux de réussite : {success/len(x_test)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Afficher la matrice de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante définie la fonction `show_cm_mnist` qui affiche la **matrice de confusion**.\n",
    "\n",
    "La matrice de confusion permet de visualiser :\n",
    "- sur la diagonale : les bonnes réponses du réseau, avec dans chaque case le nombre de bonnes réponses\n",
    "- hors diagonale : les erreurs du réseau, avec dans chaque case la fréquence d'apparition de l'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_cm_mnist(target, results, classes):\n",
    "    # target  : the actual labels (one-hot format)\n",
    "    # results : the labels computed by the trained network (one-hot format)\n",
    "    # classes : list of possible label values\n",
    "    predicted = np.argmax(results, axis=-1) # tableau d'entiers entre 0 et 9 \n",
    "    cm = confusion_matrix(target, predicted)\n",
    "    df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    plt.figure(figsize=(11,9))\n",
    "    heatmap(df_cm, annot=True, cbar=False, fmt=\"4d\")\n",
    "    plt.xlabel('actual label')\n",
    "    plt.ylabel('predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire afficher la matrice de confusion en lui passant les labels attendus `lab_test` et les labels calculés par le model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cm_mnist(lab_test, results, range(nb_classe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D/ Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Réseau profond : augmenter le nombre de couches cachées \n",
    " \n",
    " On peut essayer de construire un réseau avec plus de couches. Par exemple :\n",
    "- couche d'entrée avec les 784 pixels\n",
    "- couche cachée 1 : 500 neurones, fonction d'activation `relu`\n",
    "- couche cachée 2 : 400 neurones, fonction d'activation `relu`\n",
    "- couche de sortie : 10 neurone, fonction d'activation `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrire une fonction paramétrée `build_nn` qui effectue les opérations suivantes:\n",
    "- Construire un réseau de neurones dense dédié à la reconnaissance des chiffres MNIST avec un nombre de données d'entrée, de neurones par couche et de neurones dans la dernière couche paramétrés.\n",
    "- Entraîner le réseau avec test à la fin de chaque `epoch` et arrêt automatique basé sur la surveillance de `val_accuracy` .\n",
    "- Afficher les courbes 'Model Loss' et 'Model accuracy' du réseau entraîné.\n",
    "- Afficher la matrice de confusion.\n",
    "\n",
    "Par exemple, pour créer, entraîner et tester le réseau profond suggéré ci-dessus, on pourrait écrire :<br>\n",
    "\n",
    "    data = (x_train, y_train, x_test, y_test)\n",
    "    tf.random.set_seed(SEED)\n",
    "    my_model1 = build_mn(784, (500, 400), 10, data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction build_nn\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "from utils.tools import show_cm_mnist\n",
    "\n",
    "def build_nn(nb_input, layers, nb_classe, data, epoch=15, batch_size=64, patience=5):\n",
    "\n",
    "    x_train, y_train, x_test, y_test = data\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(nb_input,)))\n",
    "    for nb_neurone in layers:\n",
    "        model.add(Dense(nb_neurone, activation='relu'))\n",
    "    model.add(Dense(nb_classe, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "    \n",
    "    callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_accuracy',  # la grandeur à surveiller\n",
    "                  patience=patience,       # accepter que 'val_accuracy' diminue 'patience' fois de suite\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(x_train, y_train, \n",
    "                     validation_data=(x_test, y_test), \n",
    "                     epochs=epoch, \n",
    "                     batch_size=batch_size,\n",
    "                     callbacks=callbacks_list)\n",
    "    \n",
    "    plot_loss_accuracy(hist)\n",
    "    \n",
    "    show_cm_mnist(lab_test, model.predict(x_test), range(nb_classe))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisation de build_nn :\n",
    "data = (x_train, y_train, x_test, y_test)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "my_model1 = build_nn(784, (500, 400), 10, data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est l'impact de l'augmentation du nombre de couches du réseau ?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire reconnaître des images originales à un réseau entraîné avec les images MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs possibilités :\n",
    "- utiliser des images crées pour l'occasion... \n",
    "- utiliser les images toutes prêtes du répertoire `chiffres`.\n",
    "\n",
    "Si tu crées tes propres images de chiffres écrits à la main, il faut :\n",
    "- les mettre au format MNIST (28x28 pixels en ton de gris, chiffre centré dans l'image) \n",
    "- les placer dans un répertoire spécifique ,\n",
    "- affecter le nom de ce répertoire à `images_dir` dans la cellule ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# changer le nom du répertoire au besoin :\n",
    "images_dir = \"chiffres\"\n",
    "\n",
    "images = [os.path.join(images_dir,f) for f in os.listdir(images_dir) if f.endswith(\".png\")]\n",
    "images.sort()\n",
    "\n",
    "print(f\"Images du dossier <{images_dir}> à reconnaître :\")\n",
    "for im in images: print(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des fichiers image avec openCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les images doivent être convertie en images en ton de gris de 28 x 28 pixels pour pouvoir être traitées par le réseau entraîné sur les images MNIST.\n",
    "\n",
    "Plusieurs fonctions du module OpenCV pourront être utilisées :\n",
    "- `cv2.imread(file_name)` : pour lire un fichier image aux formats standards (PNG, JPG,...)\n",
    "- `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)` : pour convertir le tableau `img` renvoyé par `cv2.imread` en tons de gris\n",
    "- `cv2.resize` : pour retailler l'image.\n",
    "\n",
    "La cellule ci-dessous montre un exemple de lecture et traitement avec OpenCV des images du dossier `chiffres` qui sont déjà au format 28 x 28 pixe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "my_images = []\n",
    "for image_path in images:\n",
    "    img = cv2.imread(image_path)                    # lecture fichier image\n",
    "    img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # conversion en tons de gris\n",
    "    my_images.append(img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des images lues :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(my_images, 0, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversion des images pour avoir des chiffres doivent être en blanc sur fond noir :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_images = [255 - im for im in my_images]\n",
    "plot_images(my_images, 0, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant :\n",
    "- transformer des matrices 28x28 en vecteurs de float normalisés,\n",
    "- calculer les inférences du réseau entaîné de votre choix (`model` ou `my_model1` ou autre...) avec les images perso en entrée,\n",
    "- faire afficher la précision obtenue et la matric de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_images = np.array(my_images)\n",
    "\n",
    "nb_images = my_images.shape[0]\n",
    "nb_pixel  = my_images[0].size\n",
    "\n",
    "# mise 'à plat' des matrices sous forme de vecteurs de floats normalisés :\n",
    "my_x = my_images.reshape(nb_images, nb_pixel)/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inférences du réseau `model` avec les images perso et affichage de la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results = model.predict(my_x)\n",
    "print(my_results.argmax(axis=-1))\n",
    "target    = np.array([0,1,2,3,4,5,6,7,8,9])  # les réponses qu'on devrait avoir\n",
    "show_cm_mnist(target, my_results, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inférences du réseau `my_model1` avec les images perso et affichage de la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results = my_model1.predict(my_x)\n",
    "print(my_results.argmax(axis=-1))\n",
    "target    = np.array([0,1,2,3,4,5,6,7,8,9]) # les réponses qu'on devrait avoir\n",
    "show_cm_mnist(target, my_results, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour conclure avec les images perso il faudrait bien sûr faire des tests avec un plus grand nombre d'images....\n",
    "\n",
    "Les résultats obtenus suggèrent :\n",
    "- que le réseau à 1 couche cachée et à plusieurs couches cachées donnent des performances proches pour les images MNIST,\n",
    "- que l'apprentissage d'un réseau dense reste assez sensible à la nature du jeu de données : les images de chiffres 'maison' ne ressemblent pas tout à fait à celles de la banque MNIST, d'où des erreurs de classification avec un simple réseau dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La suite....\n",
    "\n",
    "Pour aller au-delà des 98% de reconnaissance des chiffres MNIST et une meilleure reconnaissance des 'images perso', il faut passer à une architecture de réseau plus adaptée au traitement des images : les réseaux **convolutionnels**, traités dans le notebook `TP3_MNIST_convol.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres ressources intéressantes... des vidéos :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/trWrEWfhTVg\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/trWrEWfhTVg\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"800\" height=\"450\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
